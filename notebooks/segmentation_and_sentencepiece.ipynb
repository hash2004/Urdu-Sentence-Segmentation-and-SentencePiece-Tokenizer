{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashim M Nadeem\n",
    "#### 21i-1675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# List of conjunctions in Urdu\n",
    "conjunctions = [\n",
    "    'مگر', 'لیکن', 'کیونکہ', 'بلکہ', 'چنانچہ', 'گویا', 'یعنی',\n",
    "    'اس لئے', 'تاہم', 'جبکہ', 'اگرچہ', 'حالانکہ', 'چونکہ', 'اگر',\n",
    "    'تو', 'ورنہ', 'پھر',\n",
    "    # Exclude 'اور' from conjunctions to prevent splitting\n",
    "    # Exclude 'کہ' as it's often part of other words\n",
    "]\n",
    "\n",
    "# List of sentence boundary markers (unigram, bigram, trigram)\n",
    "boundary_markers = [\n",
    "    'ہے', 'ہیں', 'تھا', 'تھی', 'تھے', 'ہوگا', 'ہوگی', 'ہوگئے', 'ہوئیں',\n",
    "    'ہوچکا', 'ہوچکی', 'گیا', 'گئی', 'گئے', 'کرے گا', 'کرے گی', 'کیا',\n",
    "    'کر چکے', 'چلا گیا', 'دے دیا', 'لگ گیا', 'کرتے ہیں',\n",
    "    'رکھا ہے', 'لیا', 'دیا', 'بن گئی', 'رکھ دی', 'کیجیے', 'کیجئے',\n",
    "    'گئیں', 'تھیں', 'ہوں', 'خریدا', 'ہونگے', 'چاہیے', 'جاسکے',\n",
    "    'بنیں', 'جائیں', 'چاہئے', 'کھایا', 'رہا', 'سکے', 'کرسکے',\n",
    "    'حوالے کئے', 'حاصل کئے', 'کروائیں گے', 'کرچکے ہیں', 'کریں گے',\n",
    "    'جائے گا', 'کیا گیا', 'جائے گی', 'کئے گئے'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Define Conjunctions & Boundary Markers\n",
    "\n",
    "### Purpose:\n",
    "This kernel sets up the initial list of conjunctions and boundary markers that are essential for performing sentence segmentation in Urdu.\n",
    "\n",
    "### Code Breakdown:\n",
    "\n",
    "1. **Imports:**\n",
    "    - `re`: This library is used for regular expression operations, essential for text processing.\n",
    "    - `unicodedata`: This library is used for Unicode character normalization, ensuring text consistency.\n",
    "\n",
    "2. **Conjunctions List:**\n",
    "    - A list of common Urdu conjunctions is defined. These conjunctions help in identifying where a sentence may be split into smaller clauses.\n",
    "    - Notably, **'اور'** (meaning \"and\") is excluded from this list to avoid splitting at \"and\", as this could break the flow of compound sentences.\n",
    "    - **'کہ'** (meaning \"that\") is also excluded since it is frequently part of other words and could lead to incorrect splitting.\n",
    "\n",
    "3. **Boundary Markers:**\n",
    "    - This list contains common **unigram, bigram, and trigram boundary markers** in Urdu that indicate the end of a sentence or a clause. \n",
    "    - The list includes verb conjugations, auxiliary verbs, and specific phrase patterns commonly used in Urdu that indicate the completion of an idea or thought.\n",
    "    - For example:\n",
    "        - 'ہے', 'تھا', 'تھی' (unigrams): Single words that often indicate sentence endings.\n",
    "        - 'کر چکے', 'کروائیں گے' (bigrams): Phrases that commonly occur at the end of clauses.\n",
    "        - 'کیا گیا', 'کئے گئے' (trigrams): More complex patterns that indicate transitions or conclusions.\n",
    "\n",
    "### Why This Approach:\n",
    "- **Conjunctions**: Used to segment compound sentences into simpler ones where appropriate, without losing meaning.\n",
    "- **Boundary Markers**: These help identify the end of clauses or sentences, essential for effective sentence segmentation.\n",
    "- The exclusions of **'اور'** and **'کہ'** are critical to avoid over-segmentation in situations where these words function more as connecting elements than boundary markers.\n",
    "\n",
    "### Challenges Addressed:\n",
    "- Urdu's sentence structure includes compound sentences with multiple clauses joined by conjunctions, so careful handling of these conjunctions and boundary markers is necessary to prevent incorrect splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:18: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/4z/b4t1_hr57n71gbffwk6cw43w0000gn/T/ipykernel_40609/646064981.py:18: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  allowed_chars = f'[{urdu_letters}{standard_punctuation}\\s]'\n"
     ]
    }
   ],
   "source": [
    "def pre_processing(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the Urdu text string, standardizes punctuation, removes unwanted characters,\n",
    "    and ensures proper spacing around punctuation marks.\n",
    "    \"\"\"\n",
    "    # Step 1: Unicode normalization\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "\n",
    "    # Step 2: Replace English punctuation with Urdu equivalents\n",
    "    text = text.replace('.', '۔')\n",
    "    text = text.replace('?', '؟')\n",
    "    text = text.replace(',', '،')\n",
    "    text = text.replace(';', '؛')\n",
    "\n",
    "    # Step 3: Preserve Urdu letters and punctuation\n",
    "    urdu_letters = r'\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF'\n",
    "    standard_punctuation = r'،؛۔!?…'\n",
    "    allowed_chars = f'[{urdu_letters}{standard_punctuation}\\s]'\n",
    "    text = re.sub(f'[^{allowed_chars}]', '', text)\n",
    "\n",
    "    # Step 4: Normalize spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Step 5: Normalize punctuation marks\n",
    "    text = re.sub(r'[!]+', '!', text)\n",
    "    text = re.sub(r'[؟]+', '؟', text)\n",
    "    text = re.sub(r'[۔]+', '۔', text)\n",
    "    text = re.sub(r'[…]+', '…', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Urdu Text for Sentence Segmentation\n",
    "\n",
    "### Purpose:\n",
    "This kernel defines the function `pre_processing()` to clean and standardize the Urdu text before segmentation. It handles Unicode normalization, punctuation standardization, and character filtering.\n",
    "\n",
    "### Code Breakdown:\n",
    "\n",
    "1. **Unicode Normalization:**\n",
    "   - The function starts by applying Unicode normalization (`NFKC` form) to the input text. This process ensures that any text inconsistencies (such as different representations of the same character) are resolved, making the text uniform.\n",
    "   - **Why**: Urdu text may contain characters that are encoded differently depending on the source, and normalizing them makes further processing more reliable.\n",
    "\n",
    "2. **Punctuation Replacement:**\n",
    "   - English punctuation marks (such as periods `.` and commas `,`) are replaced with their Urdu equivalents (`۔` and `،`). This step ensures that sentence-ending punctuation is consistent with the language's script.\n",
    "   - The function handles:\n",
    "     - **`.`** → `۔` (Urdu full stop)\n",
    "     - **`?`** → `؟` (Urdu question mark)\n",
    "     - **`,`** → `،` (Urdu comma)\n",
    "     - **`;`** → `؛` (Urdu semicolon)\n",
    "   - **Why**: Urdu texts may contain mixed punctuation from English, and replacing them helps maintain consistency in sentence boundary detection.\n",
    "\n",
    "3. **Preserving Urdu Letters and Punctuation:**\n",
    "   - A regular expression (`re.sub()`) is used to remove any non-Urdu characters from the text. Only Urdu letters (Unicode ranges `\\u0600-\\u06FF`, `\\u0750-\\u077F`, `\\u08A0-\\u08FF`) and standard punctuation marks (`،`, `؛`, `۔`, `؟`, `!`, `…`) are kept.\n",
    "   - **Why**: This step ensures that only valid Urdu script and punctuation remain in the text, eliminating any noise from other languages or symbols.\n",
    "\n",
    "4. **Space Normalization:**\n",
    "   - The function replaces multiple spaces with a single space (`\\s+ → ' '`) and removes leading or trailing spaces using `.strip()`.\n",
    "   - **Why**: Proper spacing around words and punctuation is essential for clean sentence segmentation. This step ensures there are no extraneous spaces that could interfere with text processing.\n",
    "\n",
    "5. **Punctuation Normalization:**\n",
    "   - Consecutive punctuation marks like exclamation points (`!`) and periods (`۔`) are reduced to a single instance (e.g., `!!!` becomes `!`). This prevents over-segmentation due to repeated punctuation marks.\n",
    "   - **Why**: Repeated punctuation may introduce unnecessary breaks in sentence segmentation. By normalizing these, we maintain sentence coherence.\n",
    "\n",
    "### Why This Approach:\n",
    "- **Unicode Normalization** ensures the text is consistent regardless of its origin.\n",
    "- **Punctuation Replacement** adapts English marks to their Urdu counterparts, ensuring correct sentence-ending detection.\n",
    "- **Character Filtering** ensures that the text remains focused on Urdu, avoiding noise from non-Urdu characters.\n",
    "- **Space and Punctuation Normalization** prepares the text for segmentation by standardizing spacing and punctuation usage.\n",
    "\n",
    "### Challenges Addressed:\n",
    "- Mixed language texts with English punctuation can cause incorrect sentence boundary detection.\n",
    "- Irregular spacing and non-Urdu characters can introduce noise in the segmentation process.\n",
    "- This preprocessing step is crucial to ensuring that the Urdu text is clean, standardized, and ready for further processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the text into words, handling Urdu-specific issues.\n",
    "    \"\"\"\n",
    "    # Split on spaces\n",
    "    words = text.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Function**: `tokenize_words(text)`\n",
    "- **Description**: This function takes a string of text and splits it into individual words using spaces as delimiters.\n",
    "    - **Why**: Since spaces separate words in Urdu, this function simply tokenizes based on the spaces.\n",
    "    - **Key Challenge**: Urdu words are separated by spaces, so this function doesn't require much complexity beyond splitting on spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_on_boundary_markers(text):\n",
    "    \"\"\"\n",
    "    Segments the text based on boundary markers using n-gram approach.\n",
    "    \"\"\"\n",
    "    words = tokenize_words(text)\n",
    "    segments = []\n",
    "    start = 0\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        found_boundary = False\n",
    "        # Check for n-gram boundary markers (trigram to unigram)\n",
    "        for n in range(3, 0, -1):\n",
    "            if i + 1 - n >= start:\n",
    "                ngram = ' '.join(words[i + 1 - n:i + 1])\n",
    "                if ngram in boundary_markers:\n",
    "                    # Ensure the n-gram is at the end of a clause\n",
    "                    segment = words[start:i + 1]\n",
    "                    segments.append(' '.join(segment).strip())\n",
    "                    start = i + 1\n",
    "                    found_boundary = True\n",
    "                    break  # Break inner loop if boundary is found\n",
    "        if found_boundary:\n",
    "            i = start - 1  # Adjust index after segmentation\n",
    "        i += 1\n",
    "    # Add any remaining words as a segment\n",
    "    if start < len(words):\n",
    "        segment = words[start:]\n",
    "        segments.append(' '.join(segment).strip())\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Function**: `segment_on_boundary_markers(text)`\n",
    "- **Description**: This function segments text into clauses or sentences based on the presence of boundary markers. It uses a **n-gram approach** (3-gram, bigram, and unigram) to identify sentence-ending boundaries.\n",
    "    - **How It Works**:\n",
    "        - The text is first tokenized into words using the `tokenize_words` function.\n",
    "        - The function loops through the words and checks for boundary markers (e.g., \"ہے\", \"گئے\", etc.) in a **trigram-first approach**.\n",
    "            - **Why n-grams?**: Longer sequences (trigrams) provide more context, reducing incorrect splits caused by short or ambiguous boundary markers.\n",
    "            - For example, \"گئے\" is more likely to be an actual sentence boundary than \"ہے\" when checking the surrounding words.\n",
    "        - Once an n-gram boundary is found, the words from the last boundary up to the current n-gram are added as a segment.\n",
    "        - The function continues until all words are segmented, with any remaining words being added as the final segment.\n",
    "    - **Why Use n-Grams?**\n",
    "        - **Trigram**: Provides better context, ensuring the boundary is at the end of a meaningful clause.\n",
    "        - **Bigram and Unigram**: Serve as fallbacks if no trigram is found, allowing flexibility in boundary detection.\n",
    "    - **Handling Edge Cases**:\n",
    "        - If a boundary marker is found, it ensures that it is at the end of a clause before splitting, which helps avoid unnecessary or incorrect segmentations.\n",
    "        - The loop continues, even adjusting the index (`i = start - 1`) after a segment is found, ensuring no words are skipped.\n",
    "\n",
    "### Challenges Addressed:\n",
    "\n",
    "- **Ambiguity in Sentence Boundaries**: Using n-grams helps reduce false positives caused by short boundary markers. For example, the word \"ہے\" could appear in many contexts, but by checking the surrounding words (via trigrams), we avoid over-segmentation.\n",
    "- **Urdu Specific Context**: Urdu's sentence structure often involves long clauses connected by conjunctions and boundary markers. This approach captures meaningful boundaries while minimizing false segmentation.\n",
    "\n",
    "### Key Benefits:\n",
    "\n",
    "- **Trigram Priority**: Ensures that sentence boundaries are based on the most reliable information (context).\n",
    "- **Robust to Edge Cases**: Even if no trigram boundary is found, the function falls back on bigrams and unigrams, ensuring flexibility.\n",
    "- **Efficient Handling of Words**: By breaking text into manageable segments, we ensure that each clause or sentence is meaningfully grouped for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def urdu_sentence_segmentation(text):\n",
    "    \"\"\"\n",
    "    Segments Urdu text into sentences based on punctuation marks, boundary markers, and conjunctions.\n",
    "    Implements n-gram approaches to accurately detect sentence boundaries.\n",
    "    \"\"\"\n",
    "    # Step 1: Initial segmentation based on punctuation marks\n",
    "    sentence_enders = r'[۔؟!…]'\n",
    "    sentences = re.split(f'({sentence_enders})', text)\n",
    "    initial_sentences = []\n",
    "\n",
    "    # Combine punctuation marks with sentences\n",
    "    for i in range(0, len(sentences) - 1, 2):\n",
    "        sentence = sentences[i].strip()\n",
    "        punctuation = sentences[i+1].strip()\n",
    "        full_sentence = f'{sentence}{punctuation}'\n",
    "        if full_sentence:\n",
    "            initial_sentences.append(full_sentence)\n",
    "\n",
    "    # Handle any remaining text without ending punctuation\n",
    "    if len(sentences) % 2 != 0:\n",
    "        last_sentence = sentences[-1].strip()\n",
    "        if last_sentence:\n",
    "            initial_sentences.append(last_sentence)\n",
    "\n",
    "    # Step 2: Process each sentence for compound segmentation\n",
    "    final_sentences = []\n",
    "    for sentence in initial_sentences:\n",
    "        # Tokenize the sentence into words\n",
    "        words = tokenize_words(sentence)\n",
    "\n",
    "        # Identify positions of conjunctions\n",
    "        conjunction_indices = [i for i, word in enumerate(words) if word in conjunctions]\n",
    "                # Segment at conjunctions if appropriate\n",
    "        if conjunction_indices:\n",
    "            segments = []\n",
    "            start = 0\n",
    "            for idx in conjunction_indices:\n",
    "                # Ensure the conjunction is not at the beginning\n",
    "                if idx > start:\n",
    "                    segment = words[start:idx]\n",
    "                    if segment:\n",
    "                        segments.append(' '.join(segment).strip())\n",
    "                    start = idx\n",
    "            # Add the remaining part\n",
    "            segment = words[start:]\n",
    "            if segment:\n",
    "                segments.append(' '.join(segment).strip())\n",
    "            # Process each segment further\n",
    "            for seg in segments:\n",
    "                further_segments = segment_on_boundary_markers(seg)\n",
    "                final_sentences.extend(further_segments)\n",
    "        else:\n",
    "            # No conjunctions, segment on boundary markers\n",
    "            further_segments = segment_on_boundary_markers(sentence)\n",
    "            final_sentences.extend(further_segments)\n",
    "\n",
    "    # Remove any empty strings\n",
    "    final_sentences = [s for s in final_sentences if s]\n",
    "\n",
    "    # Optional: Remove sentences that are too short (e.g., one or two words)\n",
    "    final_sentences = [s for s in final_sentences if len(s.split()) > 2]\n",
    "\n",
    "    # Step 3: Join sentences that start with 'اور' to the previous sentence\n",
    "    merged_sentences = []\n",
    "    i = 0\n",
    "    while i < len(final_sentences):\n",
    "        sentence = final_sentences[i]\n",
    "        if sentence.startswith('اور') and i > 0:\n",
    "            # Join with the previous sentence\n",
    "            merged_sentence = merged_sentences[-1] + ' ' + sentence\n",
    "            merged_sentences[-1] = merged_sentence\n",
    "        else:\n",
    "            merged_sentences.append(sentence)\n",
    "        i += 1\n",
    "\n",
    "    return merged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Urdu Sentence Segmentation with Compound Sentence Handling\n",
    "\n",
    "### Purpose:\n",
    "This kernel focuses on segmenting Urdu text into sentences based on punctuation marks, boundary markers, and conjunctions, while handling compound sentences using an n-gram approach.\n",
    "\n",
    "### Code Breakdown:\n",
    "\n",
    "### 1. Initial Segmentation Using Punctuation:\n",
    "```python\n",
    "sentence_enders = r'[۔؟!…]'\n",
    "sentences = re.split(f'({sentence_enders})', text)\n",
    "initial_sentences = []\n",
    "\n",
    "```\n",
    "\n",
    "- **Description**:\n",
    "    - **Regular Expression for Punctuation**: The `sentence_enders` variable defines common Urdu punctuation marks (e.g., \"۔\", \"؟\", \"!\"). These are used as sentence delimiters.\n",
    "    - **Splitting the Text**: The `re.split()` function splits the input text into parts, where punctuation marks separate the sentences.\n",
    "    - **Why**: Punctuation is the most basic indicator of sentence boundaries in any language, and in Urdu, marks like \"؟\" and \"۔\" are crucial for segmentation.\n",
    "\n",
    "### 2. Combine Punctuation with Sentences:\n",
    "\n",
    "```python\n",
    "for i in range(0, len(sentences) - 1, 2):\n",
    "    sentence = sentences[i].strip()\n",
    "    punctuation = sentences[i+1].strip()\n",
    "    full_sentence = f'{sentence}{punctuation}'\n",
    "    if full_sentence:\n",
    "        initial_sentences.append(full_sentence)\n",
    "\n",
    "```\n",
    "\n",
    "- **Description**:\n",
    "    - This part of the code ensures that the punctuation mark stays attached to the sentence. Every two elements from the `sentences` list (a sentence and its corresponding punctuation) are combined.\n",
    "    - **Why**: Sentences in Urdu often end with a punctuation mark, so the algorithm should merge the sentence with the punctuation to preserve meaning and structure.\n",
    "\n",
    "### 3. Handling Remaining Text Without Punctuation:\n",
    "\n",
    "```python\n",
    "if len(sentences) % 2 != 0:\n",
    "    last_sentence = sentences[-1].strip()\n",
    "    if last_sentence:\n",
    "        initial_sentences.append(last_sentence)\n",
    "\n",
    "```\n",
    "\n",
    "- **Description**:\n",
    "    - If the last portion of the text does not end with punctuation, it adds the final sentence to `initial_sentences`.\n",
    "    - **Why**: In case the text ends abruptly without punctuation, this handles that edge case by ensuring no meaningful text is discarded.\n",
    "\n",
    "### 4. Processing for Compound Sentences:\n",
    "\n",
    "```python\n",
    "conjunction_indices = [i for i, word in enumerate(words) if word in conjunctions]\n",
    "if conjunction_indices:\n",
    "    # Handle segmentation at conjunctions\n",
    "else:\n",
    "    # No conjunctions, proceed with boundary markers\n",
    "\n",
    "```\n",
    "\n",
    "- **Description**:\n",
    "    - **Conjunction-Based Segmentation**: First, the code identifies any conjunctions (like \"مگر\", \"لیکن\") within a sentence. If conjunctions are found, the sentence is split at those positions.\n",
    "    - **Why**: Compound sentences in Urdu are often joined by conjunctions, which make them longer and harder to process. By splitting these, we handle large sentences more effectively.\n",
    "\n",
    "### 5. Further Segmentation Based on Boundary Markers:\n",
    "\n",
    "```python\n",
    "further_segments = segment_on_boundary_markers(sentence)\n",
    "\n",
    "```\n",
    "\n",
    "- **Description**:\n",
    "    - If no conjunctions are found, the function falls back on boundary markers (defined by n-grams) to segment the sentence further.\n",
    "    - **Why**: Boundary markers like \"ہے\", \"تھا\", etc., are common sentence-ending elements that help in detecting sentence boundaries when conjunctions are absent.\n",
    "\n",
    "### 6. Joining Sentences Starting with 'اور':\n",
    "\n",
    "```python\n",
    "merged_sentences = []\n",
    "i = 0\n",
    "while i < len(final_sentences):\n",
    "    sentence = final_sentences[i]\n",
    "    if sentence.startswith('اور') and i > 0:\n",
    "        merged_sentence = merged_sentences[-1] + ' ' + sentence\n",
    "        merged_sentences[-1] = merged_sentence\n",
    "    else:\n",
    "        merged_sentences.append(sentence)\n",
    "    i += 1\n",
    "\n",
    "```\n",
    "\n",
    "- **Description**:\n",
    "    - **Joining Sentences with 'اور'**: Sentences that start with \"اور\" are likely continuations of previous sentences. This code merges them with the preceding sentence.\n",
    "    - **Why**: The word \"اور\" means \"and,\" which implies a continuation of thought or action from the previous sentence. By joining them, the text remains coherent and avoids unnecessary splitting.\n",
    "\n",
    "### Challenges Addressed:\n",
    "\n",
    "- **Compound Sentence Segmentation**: Urdu has long compound sentences joined by conjunctions like \"لیکن\" or \"مگر\". This approach allows us to segment them more meaningfully without losing context.\n",
    "- **Boundary Markers**: For sentences without conjunctions, boundary markers help segment based on more subtle sentence-ending clues.\n",
    "- **Handling Continuations**: Sentences starting with \"اور\" are treated as continuations of the previous sentence, avoiding improper segmentation.\n",
    "\n",
    "### Key Benefits:\n",
    "\n",
    "- **Compound Sentence Handling**: By recognizing conjunctions, we can split complex sentences into smaller, manageable parts.\n",
    "- **N-Gram Boundary Markers**: The n-gram approach ensures sentence boundaries are accurate, preventing false splits.\n",
    "- **Improved Coherence**: Joining sentences with \"اور\" maintains text coherence, crucial for Urdu text, where thoughts often span multiple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the segmented text: 75908\n"
     ]
    }
   ],
   "source": [
    "# Read the text file\n",
    "with open('/Users/hashimmuhammadnadeem/sentence/urdu-corpus.txt', 'rt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Preprocess the text\n",
    "text = pre_processing(text)\n",
    "\n",
    "# Segment the text\n",
    "segmented_text = urdu_sentence_segmentation(text)\n",
    "\n",
    "# Count the number of sentences\n",
    "sentence_count = len(segmented_text)\n",
    "print(f\"Number of sentences in the segmented text: {sentence_count}\")\n",
    "\n",
    "# Optionally, save the segmented sentences to a file\n",
    "with open('segmented_sentences.txt', 'w', encoding='utf-8') as f:\n",
    "    for sentence in segmented_text:\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:43: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:43: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/4z/b4t1_hr57n71gbffwk6cw43w0000gn/T/ipykernel_40609/1835040665.py:43: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  allowed_chars = f'[{urdu_letters}{standard_punctuation}\\s]'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the segmented text: 75908\n",
      "                                            Sentence SegmentationReason\n",
      "0        گزشتہ کئی سالوں سے مختلف بحران آتے جاتے رہے        Conjunction\n",
      "1  لیکن حالیہ آٹا ، چینی سمیت دیگر بحران اچانک پی...    Boundary Marker\n",
      "2                             تاکہ عوام کو ریلیف دیا    Boundary Marker\n",
      "3  دوسری جانب بجلی ، گیس ، پانی سمیت دیگر بلوں می...    Boundary Marker\n",
      "4  ،صوبائی و وفاقی وزراء نے اپنے اپنے ایوانوں بحر...        Punctuation\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "# List of conjunctions in Urdu\n",
    "conjunctions = [\n",
    "    'مگر', 'لیکن', 'کیونکہ', 'بلکہ', 'چنانچہ', 'گویا', 'یعنی',\n",
    "    'اس لئے', 'تاہم', 'جبکہ', 'اگرچہ', 'حالانکہ', 'چونکہ', 'اگر',\n",
    "    'تو', 'ورنہ', 'پھر',\n",
    "    # Exclude 'اور' to handle it separately\n",
    "]\n",
    "\n",
    "# List of sentence boundary markers (unigram, bigram, trigram)\n",
    "boundary_markers = [\n",
    "    'ہے', 'ہیں', 'تھا', 'تھی', 'تھے', 'ہوگا', 'ہوگی', 'ہوگئے', 'ہوئیں',\n",
    "    'ہوچکا', 'ہوچکی', 'گیا', 'گئی', 'گئے', 'کرے گا', 'کرے گی', 'کیا',\n",
    "    'کر چکے', 'چلا گیا', 'دے دیا', 'لگ گیا', 'کرتے ہیں',\n",
    "    'رکھا ہے', 'لیا', 'دیا', 'بن گئی', 'رکھ دی', 'کیجیے', 'کیجئے',\n",
    "    'گئیں', 'تھیں', 'ہوں', 'خریدا', 'ہونگے', 'چاہیے', 'جاسکے',\n",
    "    'بنیں', 'جائیں', 'چاہئے', 'کھایا', 'رہا', 'سکے', 'کرسکے',\n",
    "    'حوالے کئے', 'حاصل کئے', 'کروائیں گے', 'کرچکے ہیں', 'کریں گے',\n",
    "    'جائے گا', 'کیا گیا', 'جائے گی', 'کئے گئے'\n",
    "]\n",
    "\n",
    "def comprehensive_pre_processing(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the Urdu text string, standardizes punctuation, removes unwanted characters,\n",
    "    and ensures proper spacing around punctuation marks.\n",
    "    \"\"\"\n",
    "    # Step 1: Unicode normalization\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "\n",
    "    # Step 2: Replace English punctuation with Urdu equivalents\n",
    "    text = text.replace('.', '۔')\n",
    "    text = text.replace('?', '؟')\n",
    "    text = text.replace(',', '،')\n",
    "    text = text.replace(';', '؛')\n",
    "\n",
    "    # Step 3: Preserve Urdu letters and punctuation\n",
    "    urdu_letters = r'\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF'\n",
    "    standard_punctuation = r'،؛۔!?…'\n",
    "    allowed_chars = f'[{urdu_letters}{standard_punctuation}\\s]'\n",
    "    text = re.sub(f'[^{allowed_chars}]', '', text)\n",
    "\n",
    "    # Step 4: Normalize spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Step 5: Normalize punctuation marks\n",
    "    text = re.sub(r'[!]+', '!', text)\n",
    "    text = re.sub(r'[؟]+', '؟', text)\n",
    "    text = re.sub(r'[۔]+', '۔', text)\n",
    "    text = re.sub(r'[…]+', '…', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize_words(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the text into words, handling Urdu-specific issues.\n",
    "    \"\"\"\n",
    "    # Split on spaces\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "def urdu_sentence_segmentation(text):\n",
    "    \"\"\"\n",
    "    Segments Urdu text into sentences based on punctuation marks, boundary markers, and conjunctions.\n",
    "    Returns a list of tuples: (sentence, segmentation_reason)\n",
    "    \"\"\"\n",
    "    segmentation_results = []\n",
    "\n",
    "    # Step 1: Initial segmentation based on punctuation marks\n",
    "    sentence_enders = r'[۔؟!…]'\n",
    "    sentences = re.split(f'({sentence_enders})', text)\n",
    "    initial_sentences = []\n",
    "\n",
    "    # Combine punctuation marks with sentences\n",
    "    for i in range(0, len(sentences) - 1, 2):\n",
    "        sentence = sentences[i].strip()\n",
    "        punctuation = sentences[i+1].strip()\n",
    "        full_sentence = f'{sentence}{punctuation}'\n",
    "        if full_sentence:\n",
    "            initial_sentences.append((full_sentence, 'Punctuation'))\n",
    "\n",
    "    # Handle any remaining text without ending punctuation\n",
    "    if len(sentences) % 2 != 0:\n",
    "        last_sentence = sentences[-1].strip()\n",
    "        if last_sentence:\n",
    "            initial_sentences.append((last_sentence, 'Punctuation'))\n",
    "\n",
    "    # Step 2: Process each sentence for compound segmentation\n",
    "    final_sentences = []\n",
    "    for sentence_tuple in initial_sentences:\n",
    "        sentence, reason = sentence_tuple\n",
    "        # Tokenize the sentence into words\n",
    "        words = tokenize_words(sentence)\n",
    "\n",
    "        # Identify positions of conjunctions\n",
    "        conjunction_indices = [i for i, word in enumerate(words) if word in conjunctions]\n",
    "\n",
    "        # Segment at conjunctions if appropriate\n",
    "        if conjunction_indices:\n",
    "            segments = []\n",
    "            start = 0\n",
    "            for idx in conjunction_indices:\n",
    "                # Ensure the conjunction is not at the beginning\n",
    "                if idx > start:\n",
    "                    segment_words = words[start:idx]\n",
    "                    if segment_words:\n",
    "                        segment = ' '.join(segment_words).strip()\n",
    "                        segments.append((segment, 'Conjunction'))\n",
    "                    start = idx\n",
    "            # Add the remaining part\n",
    "            segment_words = words[start:]\n",
    "            if segment_words:\n",
    "                segment = ' '.join(segment_words).strip()\n",
    "                segments.append((segment, reason))  # Use previous reason\n",
    "            # Process each segment further\n",
    "            for seg_tuple in segments:\n",
    "                seg_sentence, seg_reason = seg_tuple\n",
    "                further_segments = segment_on_boundary_markers(seg_sentence, seg_reason)\n",
    "                final_sentences.extend(further_segments)\n",
    "        else:\n",
    "            # No conjunctions, segment on boundary markers\n",
    "            further_segments = segment_on_boundary_markers(sentence, reason)\n",
    "            final_sentences.extend(further_segments)\n",
    "\n",
    "    # Remove any empty strings\n",
    "    final_sentences = [s for s in final_sentences if s[0]]\n",
    "\n",
    "    # Optional: Remove sentences that are too short (e.g., one or two words)\n",
    "    final_sentences = [s for s in final_sentences if len(s[0].split()) > 2]\n",
    "\n",
    "    # Step 3: Join sentences that start with 'اور' to the previous sentence\n",
    "    merged_sentences = []\n",
    "    i = 0\n",
    "    while i < len(final_sentences):\n",
    "        sentence, reason = final_sentences[i]\n",
    "        if sentence.startswith('اور') and i > 0:\n",
    "            # Merge with the previous sentence\n",
    "            prev_sentence, prev_reason = merged_sentences[-1]\n",
    "            merged_sentence = prev_sentence + ' ' + sentence\n",
    "            merged_reason = prev_reason + ' + Merged with next starting with \"اور\"'\n",
    "            merged_sentences[-1] = (merged_sentence, merged_reason)\n",
    "        else:\n",
    "            merged_sentences.append((sentence, reason))\n",
    "        i += 1\n",
    "\n",
    "    return merged_sentences\n",
    "\n",
    "def segment_on_boundary_markers(text, prev_reason):\n",
    "    \"\"\"\n",
    "    Segments the text based on boundary markers using n-gram approach.\n",
    "    Returns a list of tuples: (sentence, segmentation_reason)\n",
    "    \"\"\"\n",
    "    words = tokenize_words(text)\n",
    "    segments = []\n",
    "    start = 0\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        found_boundary = False\n",
    "        # Check for n-gram boundary markers (trigram to unigram)\n",
    "        for n in range(3, 0, -1):\n",
    "            if i + 1 - n >= start:\n",
    "                ngram = ' '.join(words[i + 1 - n:i + 1])\n",
    "                if ngram in boundary_markers:\n",
    "                    # Ensure the n-gram is at the end of a clause\n",
    "                    segment_words = words[start:i + 1]\n",
    "                    segment = ' '.join(segment_words).strip()\n",
    "                    segments.append((segment, 'Boundary Marker'))\n",
    "                    start = i + 1\n",
    "                    found_boundary = True\n",
    "                    break  # Break inner loop if boundary is found\n",
    "        if found_boundary:\n",
    "            i = start - 1  # Adjust index after segmentation\n",
    "        i += 1\n",
    "    # Add any remaining words as a segment\n",
    "    if start < len(words):\n",
    "        segment_words = words[start:]\n",
    "        segment = ' '.join(segment_words).strip()\n",
    "        segments.append((segment, prev_reason))\n",
    "    return segments\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Read the text file\n",
    "    with open('/Users/hashimmuhammadnadeem/sentence/urdu-corpus.txt', 'rt', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Preprocess the text\n",
    "    text = comprehensive_pre_processing(text)\n",
    "\n",
    "    # Segment the text\n",
    "    segmented_sentences = urdu_sentence_segmentation(text)\n",
    "\n",
    "    # Create a DataFrame with sentences and segmentation reasons\n",
    "    df = pd.DataFrame(segmented_sentences, columns=['Sentence', 'SegmentationReason'])\n",
    "\n",
    "    # Count the number of sentences\n",
    "    sentence_count = len(df)\n",
    "    print(f\"Number of sentences in the segmented text: {sentence_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same code, except it does it in a dataframe instead, and uses another column called SegmentationReason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>SegmentationReason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>گزشتہ کئی سالوں سے مختلف بحران آتے جاتے رہے</td>\n",
       "      <td>Conjunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>لیکن حالیہ آٹا ، چینی سمیت دیگر بحران اچانک پی...</td>\n",
       "      <td>Boundary Marker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تاکہ عوام کو ریلیف دیا</td>\n",
       "      <td>Boundary Marker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>دوسری جانب بجلی ، گیس ، پانی سمیت دیگر بلوں می...</td>\n",
       "      <td>Boundary Marker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>،صوبائی و وفاقی وزراء نے اپنے اپنے ایوانوں بحر...</td>\n",
       "      <td>Punctuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75903</th>\n",
       "      <td>کیا22سالہ جدوجہداورقربانیوں کامنطقی مقصدفقت یہ...</td>\n",
       "      <td>Punctuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75904</th>\n",
       "      <td>اب دیکھنایہ ہے</td>\n",
       "      <td>Boundary Marker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75905</th>\n",
       "      <td>کہ عمران خان نے الیکشن سے پہلے جوبھڑکیں ماری و...</td>\n",
       "      <td>Boundary Marker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75906</th>\n",
       "      <td>یاصرف عوام کواذیت دے کے رخصت ہوجاتے ہیں</td>\n",
       "      <td>Boundary Marker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75907</th>\n",
       "      <td>جو تیرے وعدے پہ اعتبار کیا</td>\n",
       "      <td>Boundary Marker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75908 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence SegmentationReason\n",
       "0            گزشتہ کئی سالوں سے مختلف بحران آتے جاتے رہے        Conjunction\n",
       "1      لیکن حالیہ آٹا ، چینی سمیت دیگر بحران اچانک پی...    Boundary Marker\n",
       "2                                 تاکہ عوام کو ریلیف دیا    Boundary Marker\n",
       "3      دوسری جانب بجلی ، گیس ، پانی سمیت دیگر بلوں می...    Boundary Marker\n",
       "4      ،صوبائی و وفاقی وزراء نے اپنے اپنے ایوانوں بحر...        Punctuation\n",
       "...                                                  ...                ...\n",
       "75903  کیا22سالہ جدوجہداورقربانیوں کامنطقی مقصدفقت یہ...        Punctuation\n",
       "75904                                     اب دیکھنایہ ہے    Boundary Marker\n",
       "75905  کہ عمران خان نے الیکشن سے پہلے جوبھڑکیں ماری و...    Boundary Marker\n",
       "75906            یاصرف عوام کواذیت دے کے رخصت ہوجاتے ہیں    Boundary Marker\n",
       "75907                         جو تیرے وعدے پہ اعتبار کیا    Boundary Marker\n",
       "\n",
       "[75908 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>SegmentationReason</th>\n",
       "      <th>BPE_Tokens</th>\n",
       "      <th>Merges</th>\n",
       "      <th>Decoded_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>گزشتہ کئی سالوں سے مختلف بحران آتے جاتے رہے</td>\n",
       "      <td>Conjunction</td>\n",
       "      <td>[گ, ز, ش, ت, ہ,  , ک, ئ, ی, 257, ا, ل, و, ں, 2...</td>\n",
       "      <td>{('ے', ' '): 256, (' ', 'س'): 257, ('ت', 256):...</td>\n",
       "      <td>گزشتہ کئی سالوں سے مختلف بحران آتے جاتے رہے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>لیکن حالیہ آٹا ، چینی سمیت دیگر بحران اچانک پی...</td>\n",
       "      <td>Boundary Marker</td>\n",
       "      <td>[259, ک, 260, ح, ا, 259, ہ, 261, ٹ, ا,  , ،,  ...</td>\n",
       "      <td>{(' ', 'ا'): 256, ('ی', ' '): 257, (' ', 'د'):...</td>\n",
       "      <td>لیکن حالیہ آٹا ، چینی سمیت دیگر بحران اچانک پی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تاکہ عوام کو ریلیف دیا</td>\n",
       "      <td>Boundary Marker</td>\n",
       "      <td>[ت, ا, ک, ہ,  , ع, و, ا, م,  , ک, و,  , ر, ی, ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>تاکہ عوام کو ریلیف دیا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>دوسری جانب بجلی ، گیس ، پانی سمیت دیگر بلوں می...</td>\n",
       "      <td>Boundary Marker</td>\n",
       "      <td>[د, و, س, ر, 256, ج, 258, ب, 259, ج, ل, 256, ،...</td>\n",
       "      <td>{('ی', ' '): 256, (' ', 'گ'): 257, ('ا', 'ن'):...</td>\n",
       "      <td>دوسری جانب بجلی ، گیس ، پانی سمیت دیگر بلوں می...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>،صوبائی و وفاقی وزراء نے اپنے اپنے ایوانوں بحر...</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>[،, ص, و, ب, 260, 263,  , و, ف, ا, ق, 263, ز, ...</td>\n",
       "      <td>{('ے', ' '): 256, ('ی', ' '): 257, (' ', 'ک'):...</td>\n",
       "      <td>،صوبائی و وفاقی وزراء نے اپنے اپنے ایوانوں بحر...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75903</th>\n",
       "      <td>کیا22سالہ جدوجہداورقربانیوں کامنطقی مقصدفقت یہ...</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>[ک, ی, ا, 2, 2, س, ا, ل, ہ,  , ج, د, و, ج, ہ, ...</td>\n",
       "      <td>{('ی', ' '): 256, ('ت', ' '): 257}</td>\n",
       "      <td>کیا22سالہ جدوجہداورقربانیوں کامنطقی مقصدفقت یہ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75904</th>\n",
       "      <td>اب دیکھنایہ ہے</td>\n",
       "      <td>Boundary Marker</td>\n",
       "      <td>[ا, ب,  , د, ی, ک, ھ, ن, ا, ی, ہ,  , ہ, ے]</td>\n",
       "      <td>{}</td>\n",
       "      <td>اب دیکھنایہ ہے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75905</th>\n",
       "      <td>کہ عمران خان نے الیکشن سے پہلے جوبھڑکیں ماری و...</td>\n",
       "      <td>Boundary Marker</td>\n",
       "      <td>[ک, 258, ع, م, ر, 259, خ, 259, ن, 256, ا, ل, ی...</td>\n",
       "      <td>{('ے', ' '): 256, ('ن', ' '): 257, ('ہ', ' '):...</td>\n",
       "      <td>کہ عمران خان نے الیکشن سے پہلے جوبھڑکیں ماری و...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75906</th>\n",
       "      <td>یاصرف عوام کواذیت دے کے رخصت ہوجاتے ہیں</td>\n",
       "      <td>Boundary Marker</td>\n",
       "      <td>[ی, ا, ص, ر, ف,  , ع, 257, م,  , ک, 257, ذ, ی,...</td>\n",
       "      <td>{('ے', ' '): 256, ('و', 'ا'): 257, ('ت', ' '):...</td>\n",
       "      <td>یاصرف عوام کواذیت دے کے رخصت ہوجاتے ہیں</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75907</th>\n",
       "      <td>جو تیرے وعدے پہ اعتبار کیا</td>\n",
       "      <td>Boundary Marker</td>\n",
       "      <td>[ج, و,  , ت, ی, ر, 256, و, ع, د, 256, پ, ہ,  ,...</td>\n",
       "      <td>{('ے', ' '): 256}</td>\n",
       "      <td>جو تیرے وعدے پہ اعتبار کیا</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75908 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence SegmentationReason  \\\n",
       "0            گزشتہ کئی سالوں سے مختلف بحران آتے جاتے رہے        Conjunction   \n",
       "1      لیکن حالیہ آٹا ، چینی سمیت دیگر بحران اچانک پی...    Boundary Marker   \n",
       "2                                 تاکہ عوام کو ریلیف دیا    Boundary Marker   \n",
       "3      دوسری جانب بجلی ، گیس ، پانی سمیت دیگر بلوں می...    Boundary Marker   \n",
       "4      ،صوبائی و وفاقی وزراء نے اپنے اپنے ایوانوں بحر...        Punctuation   \n",
       "...                                                  ...                ...   \n",
       "75903  کیا22سالہ جدوجہداورقربانیوں کامنطقی مقصدفقت یہ...        Punctuation   \n",
       "75904                                     اب دیکھنایہ ہے    Boundary Marker   \n",
       "75905  کہ عمران خان نے الیکشن سے پہلے جوبھڑکیں ماری و...    Boundary Marker   \n",
       "75906            یاصرف عوام کواذیت دے کے رخصت ہوجاتے ہیں    Boundary Marker   \n",
       "75907                         جو تیرے وعدے پہ اعتبار کیا    Boundary Marker   \n",
       "\n",
       "                                              BPE_Tokens  \\\n",
       "0      [گ, ز, ش, ت, ہ,  , ک, ئ, ی, 257, ا, ل, و, ں, 2...   \n",
       "1      [259, ک, 260, ح, ا, 259, ہ, 261, ٹ, ا,  , ،,  ...   \n",
       "2      [ت, ا, ک, ہ,  , ع, و, ا, م,  , ک, و,  , ر, ی, ...   \n",
       "3      [د, و, س, ر, 256, ج, 258, ب, 259, ج, ل, 256, ،...   \n",
       "4      [،, ص, و, ب, 260, 263,  , و, ف, ا, ق, 263, ز, ...   \n",
       "...                                                  ...   \n",
       "75903  [ک, ی, ا, 2, 2, س, ا, ل, ہ,  , ج, د, و, ج, ہ, ...   \n",
       "75904         [ا, ب,  , د, ی, ک, ھ, ن, ا, ی, ہ,  , ہ, ے]   \n",
       "75905  [ک, 258, ع, م, ر, 259, خ, 259, ن, 256, ا, ل, ی...   \n",
       "75906  [ی, ا, ص, ر, ف,  , ع, 257, م,  , ک, 257, ذ, ی,...   \n",
       "75907  [ج, و,  , ت, ی, ر, 256, و, ع, د, 256, پ, ہ,  ,...   \n",
       "\n",
       "                                                  Merges  \\\n",
       "0      {('ے', ' '): 256, (' ', 'س'): 257, ('ت', 256):...   \n",
       "1      {(' ', 'ا'): 256, ('ی', ' '): 257, (' ', 'د'):...   \n",
       "2                                                     {}   \n",
       "3      {('ی', ' '): 256, (' ', 'گ'): 257, ('ا', 'ن'):...   \n",
       "4      {('ے', ' '): 256, ('ی', ' '): 257, (' ', 'ک'):...   \n",
       "...                                                  ...   \n",
       "75903                 {('ی', ' '): 256, ('ت', ' '): 257}   \n",
       "75904                                                 {}   \n",
       "75905  {('ے', ' '): 256, ('ن', ' '): 257, ('ہ', ' '):...   \n",
       "75906  {('ے', ' '): 256, ('و', 'ا'): 257, ('ت', ' '):...   \n",
       "75907                                  {('ے', ' '): 256}   \n",
       "\n",
       "                                        Decoded_Sentence  \n",
       "0            گزشتہ کئی سالوں سے مختلف بحران آتے جاتے رہے  \n",
       "1      لیکن حالیہ آٹا ، چینی سمیت دیگر بحران اچانک پی...  \n",
       "2                                 تاکہ عوام کو ریلیف دیا  \n",
       "3      دوسری جانب بجلی ، گیس ، پانی سمیت دیگر بلوں می...  \n",
       "4      ،صوبائی و وفاقی وزراء نے اپنے اپنے ایوانوں بحر...  \n",
       "...                                                  ...  \n",
       "75903  کیا22سالہ جدوجہداورقربانیوں کامنطقی مقصدفقت یہ...  \n",
       "75904                                     اب دیکھنایہ ہے  \n",
       "75905  کہ عمران خان نے الیکشن سے پہلے جوبھڑکیں ماری و...  \n",
       "75906            یاصرف عوام کواذیت دے کے رخصت ہوجاتے ہیں  \n",
       "75907                         جو تیرے وعدے پہ اعتبار کیا  \n",
       "\n",
       "[75908 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your BPE functions from above\n",
    "def get_pair_freq(corpus):\n",
    "    \"\"\"Calculate the frequency of consecutive pairs of tokens in the corpus.\"\"\"\n",
    "    counts = {}\n",
    "    for i in range(len(corpus) - 1):\n",
    "        pair = (corpus[i], corpus[i + 1])\n",
    "        if pair in counts:\n",
    "            counts[pair] += 1\n",
    "        else:\n",
    "            counts[pair] = 1\n",
    "    return counts\n",
    "\n",
    "def merge(cipher, pair, new_token):\n",
    "    \"\"\"Merge a given pair in the cipher and replace it with a new token.\"\"\"\n",
    "    new_cipher = []\n",
    "    i = 0\n",
    "    while i < len(cipher):\n",
    "        if i < len(cipher) - 1 and cipher[i] == pair[0] and cipher[i + 1] == pair[1]:\n",
    "            new_cipher.append(new_token)\n",
    "            i += 2  # Skip over the merged pair\n",
    "        else:\n",
    "            new_cipher.append(cipher[i])\n",
    "            i += 1\n",
    "    return new_cipher\n",
    "\n",
    "def encode(text, max_merges=40):\n",
    "    \"\"\"Encode the text using BPE, returning the merged cipher and the vocabulary.\"\"\"\n",
    "    corpus = list(text)  # List of characters/tokens\n",
    "    cipher = corpus[:]\n",
    "    merges = {}\n",
    "    token_counter = 256  # Start new tokens at 256 (ASCII range ends at 255)\n",
    "\n",
    "    # Perform merging up to max_merges times\n",
    "    for _ in range(max_merges):\n",
    "        pair_freq = get_pair_freq(cipher)\n",
    "        \n",
    "        # If no frequent pairs, stop the merge process\n",
    "        if not pair_freq:\n",
    "            break\n",
    "\n",
    "        # Find the most frequent pair\n",
    "        most_frequent_pair = max(pair_freq, key=pair_freq.get)\n",
    "\n",
    "        # Only merge if the pair frequency is greater than 1 to avoid merging sparse pairs\n",
    "        if pair_freq[most_frequent_pair] <= 1:\n",
    "            break\n",
    "\n",
    "        # Assign a new token for this pair\n",
    "        new_token = token_counter\n",
    "        token_counter += 1\n",
    "\n",
    "        # Update cipher with the merged token\n",
    "        cipher = merge(cipher, most_frequent_pair, new_token)\n",
    "        merges[most_frequent_pair] = new_token\n",
    "\n",
    "    return cipher, merges\n",
    "\n",
    "def decode(cipher, merges):\n",
    "    \"\"\"Decode the BPE-encoded cipher back into the original text using the merges.\"\"\"\n",
    "    reverse_merges = {v: k for k, v in merges.items()}\n",
    "    \n",
    "    # Decoding step by step, replacing tokens with original pairs\n",
    "    decoded_text = cipher[:]\n",
    "    made_replacement = True\n",
    "    \n",
    "    while made_replacement:\n",
    "        made_replacement = False\n",
    "        new_decoded_text = []\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(decoded_text):\n",
    "            if decoded_text[i] in reverse_merges:\n",
    "                # Replace token with original pair\n",
    "                pair = reverse_merges[decoded_text[i]]\n",
    "                new_decoded_text.append(pair[0])\n",
    "                new_decoded_text.append(pair[1])\n",
    "                made_replacement = True\n",
    "            else:\n",
    "                new_decoded_text.append(decoded_text[i])\n",
    "            i += 1\n",
    "\n",
    "        decoded_text = new_decoded_text\n",
    "\n",
    "    return ''.join(chr(t) if isinstance(t, int) else t for t in decoded_text)\n",
    "\n",
    "# Function to apply BPE encoding to each sentence\n",
    "def apply_bpe_to_sentence(sentence):\n",
    "    cipher, merges = encode(sentence)\n",
    "    return cipher, merges  # Returning both the BPE-encoded sentence (cipher) and merges\n",
    "\n",
    "# Function to decode a BPE-encoded sentence\n",
    "def decode_bpe_sentence(bpe_tokens, merges):\n",
    "    return decode(bpe_tokens, merges)\n",
    "\n",
    "\n",
    "# Apply the BPE encoding to the 'Sentence' column and store both the tokens and merges\n",
    "df[['BPE_Tokens', 'Merges']] = df['Sentence'].apply(lambda x: pd.Series(apply_bpe_to_sentence(x)))\n",
    "\n",
    "# Apply the decoding process to get the decoded sentences\n",
    "df['Decoded_Sentence'] = df.apply(lambda row: decode_bpe_sentence(row['BPE_Tokens'], row['Merges']), axis=1)\n",
    "\n",
    "# Display the updated DataFrame with BPE tokens and decoded sentences\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding (BPE) for Urdu Sentences\n",
    "\n",
    "### Purpose:\n",
    "This kernel introduces the Byte Pair Encoding (BPE) algorithm for encoding and decoding sentences. It handles Urdu sentences by representing frequent pairs of tokens with unique, compact tokens, which makes it more efficient for downstream tasks like tokenization.\n",
    "\n",
    "### Code Breakdown:\n",
    "\n",
    "### 1. Calculating the Frequency of Consecutive Token Pairs:\n",
    "```python\n",
    "def get_pair_freq(corpus):\n",
    "    counts = {}\n",
    "    for i in range(len(corpus) - 1):\n",
    "        pair = (corpus[i], corpus[i + 1])\n",
    "        if pair in counts:\n",
    "            counts[pair] += 1\n",
    "        else:\n",
    "            counts[pair] = 1\n",
    "    return counts\n",
    "\n",
    "```\n",
    "\n",
    "- **Description**:\n",
    "    - **Token Pair Frequency Calculation**: This function scans the corpus (text represented as a list of characters or tokens) and counts how often each consecutive pair of tokens appears.\n",
    "    - **Why**: In BPE, frequent pairs are merged into new tokens, so it's necessary to first calculate the frequency of consecutive pairs.\n",
    "\n",
    "### 2. Merging the Most Frequent Pair:\n",
    "\n",
    "```python\n",
    "def merge(cipher, pair, new_token):\n",
    "    new_cipher = []\n",
    "    i = 0\n",
    "    while i < len(cipher):\n",
    "        if i < len(cipher) - 1 and cipher[i] == pair[0] and cipher[i + 1] == pair[1]:\n",
    "            new_cipher.append(new_token)\n",
    "            i += 2  # Skip over the merged pair\n",
    "        else:\n",
    "            new_cipher.append(cipher[i])\n",
    "            i += 1\n",
    "    return new_cipher\n",
    "\n",
    "```\n",
    "\n",
    "- **Description**:\n",
    "    - **Merging Frequent Pairs**: The function replaces the most frequent pair with a new token in the text (cipher), effectively reducing the number of tokens in the text.\n",
    "    - **Why**: By merging frequent pairs, the algorithm reduces the number of tokens and creates a more compact representation of the text.\n",
    "\n",
    "### 3. Encoding Text Using BPE:\n",
    "\n",
    "```python\n",
    "def encode(text, max_merges=40):\n",
    "    corpus = list(text)  # List of characters/tokens\n",
    "    cipher = corpus[:]\n",
    "    merges = {}\n",
    "    token_counter = 256  # Start new tokens at 256 (ASCII range ends at 255)\n",
    "\n",
    "    for _ in range(max_merges):\n",
    "        pair_freq = get_pair_freq(cipher)\n",
    "\n",
    "        if not pair_freq:\n",
    "            break\n",
    "\n",
    "        most_frequent_pair = max(pair_freq, key=pair_freq.get)\n",
    "\n",
    "        if pair_freq[most_frequent_pair] <= 1:\n",
    "            break\n",
    "\n",
    "        new_token = token_counter\n",
    "        token_counter += 1\n",
    "\n",
    "        cipher = merge(cipher, most_frequent_pair, new_token)\n",
    "        merges[most_frequent_pair] = new_token\n",
    "\n",
    "    return cipher, merges\n",
    "\n",
    "```\n",
    "\n",
    "- **Description**:\n",
    "    - **BPE Encoding**: This function performs BPE by repeatedly merging frequent pairs up to a maximum of 40 times. The function returns the encoded text (cipher) and a dictionary of merges.\n",
    "    - **Why**: The BPE algorithm transforms the input text into a compact representation, allowing us to handle the tokenization efficiently for Urdu, where compound words and phrases are common.\n",
    "\n",
    "### 4. Decoding BPE-Encoded Text:\n",
    "\n",
    "```python\n",
    "def decode(cipher, merges):\n",
    "    reverse_merges = {v: k for k, v in merges.items()}\n",
    "\n",
    "    decoded_text = cipher[:]\n",
    "    made_replacement = True\n",
    "\n",
    "    while made_replacement:\n",
    "        made_replacement = False\n",
    "        new_decoded_text = []\n",
    "\n",
    "        i = 0\n",
    "        while i < len(decoded_text):\n",
    "            if decoded_text[i] in reverse_merges:\n",
    "                pair = reverse_merges[decoded_text[i]]\n",
    "                new_decoded_text.append(pair[0])\n",
    "                new_decoded_text.append(pair[1])\n",
    "                made_replacement = True\n",
    "            else:\n",
    "                new_decoded_text.append(decoded_text[i])\n",
    "            i += 1\n",
    "\n",
    "        decoded_text = new_decoded_text\n",
    "\n",
    "    return ''.join(chr(t) if isinstance(t, int) else t for t in decoded_text)\n",
    "\n",
    "```\n",
    "\n",
    "- **Description**:\n",
    "    - **Decoding Process**: The decoding function reverses the BPE process by repeatedly replacing the tokens with their original pairs until the text is fully restored.\n",
    "    - **Why**: This allows us to verify the correctness of the BPE encoding by decoding the compact representation back into its original form.\n",
    "\n",
    "### 5. Applying BPE to Each Sentence:\n",
    "\n",
    "```python\n",
    "def apply_bpe_to_sentence(sentence):\n",
    "    cipher, merges = encode(sentence)\n",
    "    return cipher, merges\n",
    "\n",
    "```\n",
    "\n",
    "- **Description**:\n",
    "    - **Apply BPE**: This function applies the BPE encoding to an individual sentence and returns both the encoded sentence (cipher) and the merges.\n",
    "    - **Why**: Each sentence in the dataset needs to be processed separately, and this function ensures that BPE is applied correctly to each sentence.\n",
    "\n",
    "### 6. Applying the Encoding and Decoding:\n",
    "\n",
    "```python\n",
    "df[['BPE_Tokens', 'Merges']] = df['Sentence'].apply(lambda x: pd.Series(apply_bpe_to_sentence(x)))\n",
    "df['Decoded_Sentence'] = df.apply(lambda row: decode_bpe_sentence(row['BPE_Tokens'], row['Merges']), axis=1)\n",
    "\n",
    "```\n",
    "\n",
    "- **Description**:\n",
    "    - **BPE Tokenization**: The sentences in the dataset are encoded using BPE, and the resulting tokens and merges are stored in two new columns: `BPE_Tokens` and `Merges`.\n",
    "    - **Decoding**: The encoded sentences are then decoded using the merges, and the decoded sentences are stored in a new column `Decoded_Sentence`.\n",
    "    - **Why**: These steps allow us to apply and evaluate the BPE encoding process, ensuring that the original sentences can be encoded and decoded accurately.\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **Efficiency**: BPE efficiently compresses common subwords, reducing the total number of tokens and making it more suitable for downstream NLP tasks like machine translation or language modeling.\n",
    "- **Reversibility**: The BPE encoding is fully reversible, allowing us to verify the results by decoding the tokens back into the original sentences.\n",
    "- **Handling of Urdu Script**: The BPE algorithm is capable of handling Urdu script efficiently, especially in cases where common subwords (e.g., roots and suffixes) need to be compactly represented.\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "sentences = df['Sentence'].tolist()\n",
    "\n",
    "# Set the maximum and minimum subword lengths\n",
    "max_subword_length = 10  # Increased to allow longer subwords\n",
    "min_subword_length = 3   # Set minimum subword length to 3 characters\n",
    "\n",
    "# Function to generate subwords based on trigrams, with minimum subword length\n",
    "def generate_subwords(sentence):\n",
    "    subwords = set()\n",
    "    sentence = sentence.replace(' ', '')  # Remove spaces for subword generation\n",
    "    length = len(sentence)\n",
    "    for i in range(length):\n",
    "        for j in range(min_subword_length, min(max_subword_length + 1, length - i + 1)):\n",
    "            subword = sentence[i:i+j]\n",
    "            subwords.add(subword)\n",
    "    return subwords\n",
    "\n",
    "# Build the initial vocabulary\n",
    "vocab = set()\n",
    "for sentence in sentences:\n",
    "    subwords = generate_subwords(sentence)\n",
    "    vocab.update(subwords)\n",
    "vocab = list(vocab)\n",
    "\n",
    "# Calculate character unigram and trigram frequencies\n",
    "char_unigram_counts = defaultdict(int)\n",
    "char_trigram_counts = defaultdict(int)\n",
    "total_chars = 0\n",
    "total_trigrams = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_no_spaces = sentence.replace(' ', '')\n",
    "    length = len(sentence_no_spaces)\n",
    "    # Update unigram counts\n",
    "    for i in range(length):\n",
    "        char = sentence_no_spaces[i]\n",
    "        char_unigram_counts[char] += 1\n",
    "        total_chars += 1\n",
    "    # Update trigram counts\n",
    "    for i in range(length - 2):\n",
    "        trigram = sentence_no_spaces[i:i+3]\n",
    "        char_trigram_counts[trigram] += 1\n",
    "        total_trigrams += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword Generation and Frequency Calculation\n",
    "\n",
    "#### Key Changes:\n",
    "- **Maximum Subword Length**: We increased the `max_subword_length` to **10** characters. This allows the model to capture longer subwords, which can help represent meaningful words or phrases without over-segmentation.\n",
    "  \n",
    "- **Minimum Subword Length**: Introduced a `min_subword_length` of **3** characters. This avoids generating tokens that are too short to be useful (e.g., single characters or digraphs), reducing the likelihood of meaningless subword representations.\n",
    "  \n",
    "#### Step-by-Step Process:\n",
    "\n",
    "1. **Remove Spaces**: \n",
    "   - Spaces are removed from the sentence to prevent them from breaking up subwords.\n",
    "\n",
    "2. **Subword Generation**: \n",
    "   - We iterate through each character in the sentence and generate subwords of length ranging from the `min_subword_length` (3) to the `max_subword_length` (10). This ensures that only meaningful subwords are generated. \n",
    "\n",
    "3. **Building Vocabulary**:\n",
    "   - A set called `vocab` is updated with all generated subwords. The vocabulary contains all unique subwords that the model will use for tokenization.\n",
    "\n",
    "4. **Unigram and Trigram Frequency Calculation**:\n",
    "   - **Unigram Counts**: We iterate over each sentence to count the frequency of individual characters (unigrams). This provides the probability distribution of individual characters.\n",
    "   - **Trigram Counts**: We also count character trigrams. This captures dependencies between characters and helps the model assign better probabilities to multi-character subwords.\n",
    "   \n",
    "5. **Totals**:\n",
    "   - `total_chars` tracks the total number of characters (unigrams), while `total_trigrams` tracks the total number of trigrams across all sentences. These totals are used to normalize the probabilities later.\n",
    "\n",
    "#### Purpose:\n",
    "- This kernel generates a **dynamic vocabulary** of subwords based on the sentence data and calculates the frequencies of unigrams and trigrams. The use of both unigrams and trigrams ensures that the tokenization process can capture meaningful subword patterns, while the length constraints prevent over-segmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute character unigram and trigram probabilities\n",
    "char_unigram_probs = {}\n",
    "for char, count in char_unigram_counts.items():\n",
    "    char_unigram_probs[char] = count / total_chars\n",
    "\n",
    "char_trigram_probs = {}\n",
    "for trigram, count in char_trigram_counts.items():\n",
    "    char_trigram_probs[trigram] = count / total_trigrams\n",
    "\n",
    "# Function to compute subword log probabilities based on character trigrams\n",
    "def compute_subword_log_probability(subword, char_unigram_probs, char_trigram_probs):\n",
    "    if len(subword) < 3:\n",
    "        # For subwords shorter than 3 characters, use character unigram probabilities\n",
    "        log_prob = 0.0\n",
    "        for char in subword:\n",
    "            char_prob = char_unigram_probs.get(char, 1e-8)  # Small probability for unseen chars\n",
    "            log_prob += math.log(char_prob)\n",
    "        return log_prob\n",
    "    else:\n",
    "        # For subwords of length >= 3, use character trigram probabilities\n",
    "        log_prob = 0.0\n",
    "        for i in range(len(subword) - 2):\n",
    "            trigram = subword[i:i+3]\n",
    "            trigram_prob = char_trigram_probs.get(trigram, 1e-8)  # Small probability for unseen trigrams\n",
    "            log_prob += math.log(trigram_prob)\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Explanation: Probability Computation for Unigrams and Trigrams\n",
    "\n",
    "#### Step-by-Step Process:\n",
    "\n",
    "1. **Unigram Probability Calculation**:\n",
    "   - For each character in the dataset, we compute its probability by dividing the character's frequency count (`char_unigram_counts[char]`) by the total number of characters (`total_chars`). \n",
    "   - This provides a normalized probability distribution for unigrams, stored in the dictionary `char_unigram_probs`.\n",
    "\n",
    "2. **Trigram Probability Calculation**:\n",
    "   - Similarly, for each trigram in the dataset, we calculate its probability by dividing the trigram's frequency count (`char_trigram_counts[trigram]`) by the total number of trigrams (`total_trigrams`).\n",
    "   - This normalized distribution for trigrams is stored in the dictionary `char_trigram_probs`.\n",
    "\n",
    "3. **Log Probability for Subwords**:\n",
    "   - The function `compute_subword_log_probability` calculates the log probability for a given subword based on its length:\n",
    "     \n",
    "     a. **For subwords shorter than 3 characters**:\n",
    "        - The unigram probabilities are used. Each character in the subword is assigned a probability from `char_unigram_probs`. If a character is unseen (i.e., not in the unigram dictionary), a small default probability (`1e-8`) is used to prevent division by zero. The log probability is then accumulated for all characters in the subword.\n",
    "     \n",
    "     b. **For subwords of 3 characters or longer**:\n",
    "        - The trigram probabilities are applied. For each trigram within the subword, its probability is fetched from `char_trigram_probs`. Similar to unigrams, unseen trigrams are assigned a small probability (`1e-8`). The log probability of the subword is the sum of the log probabilities of its constituent trigrams.\n",
    "\n",
    "#### Purpose:\n",
    "- **Log Probability Calculation**:\n",
    "   - Log probabilities are used because they are numerically more stable when dealing with the small probabilities common in natural language processing. \n",
    "   \n",
    "- **Handling Short vs. Long Subwords**:\n",
    "   - For short subwords (length < 3), unigram probabilities are sufficient. However, longer subwords benefit from trigram probabilities, as trigrams capture richer contextual information and relationships between characters.\n",
    "\n",
    "#### Why This Matters:\n",
    "- **Efficient Tokenization**:\n",
    "   - Using character probabilities allows the model to make informed decisions about the likelihood of different subwords, leading to more meaningful tokenization.\n",
    "   \n",
    "- **Generalization**:\n",
    "   - By applying small default probabilities to unseen characters or trigrams, the model can generalize better to previously unseen data without breaking.\n",
    "\n",
    "#### Key Adjustments:\n",
    "- **Minimum Subword Length Handling**:\n",
    "   - Unigram probabilities are used for subwords shorter than 3 characters, while trigram probabilities handle longer subwords to provide a robust segmentation strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E-step function using Viterbi algorithm, adjusted for minimum subword length\n",
    "def e_step(sentences, char_unigram_probs, char_trigram_probs, min_length, max_length):\n",
    "    total_log_likelihood = 0.0\n",
    "    tokenizations = []\n",
    "    expected_char_trigram_counts = defaultdict(float)\n",
    "    expected_char_unigram_counts = defaultdict(float)\n",
    "    for sentence in sentences:\n",
    "        sentence_no_spaces = sentence.replace(' ', '')\n",
    "        length = len(sentence_no_spaces)\n",
    "        best_log_probs = [float('-inf')] * (length + 1)\n",
    "        best_log_probs[0] = 0.0\n",
    "        back_pointers = [0] * (length + 1)\n",
    "\n",
    "        for i in range(1, length + 1):\n",
    "            # Adjust j to start from min_length\n",
    "            for j in range(max(0, i - max_length), i - min_length + 1):\n",
    "                subword = sentence_no_spaces[j:i]\n",
    "                if len(subword) >= min_length:\n",
    "                    log_prob = compute_subword_log_probability(subword, char_unigram_probs, char_trigram_probs)\n",
    "                    if log_prob + best_log_probs[j] > best_log_probs[i]:\n",
    "                        best_log_probs[i] = log_prob + best_log_probs[j]\n",
    "                        back_pointers[i] = j\n",
    "\n",
    "        total_log_likelihood += best_log_probs[length]\n",
    "        # Recover the best path and update expected counts\n",
    "        i = length\n",
    "        tokens = []\n",
    "        while i > 0:\n",
    "            j = back_pointers[i]\n",
    "            subword = sentence_no_spaces[j:i]\n",
    "            tokens.insert(0, subword)\n",
    "            # Update expected counts\n",
    "            if len(subword) >= 3:\n",
    "                for k in range(len(subword) - 2):\n",
    "                    trigram = subword[k:k+3]\n",
    "                    expected_char_trigram_counts[trigram] += 1\n",
    "            else:\n",
    "                for char in subword:\n",
    "                    expected_char_unigram_counts[char] += 1\n",
    "            i = j\n",
    "\n",
    "        tokenizations.append(tokens)\n",
    "\n",
    "    return tokenizations, total_log_likelihood, expected_char_unigram_counts, expected_char_trigram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Explanation: E-step Function Using Viterbi Algorithm (Adjusted for Minimum Subword Length)\n",
    "\n",
    "#### Step-by-Step Process:\n",
    "\n",
    "1. **Viterbi Algorithm for Segmentation**:\n",
    "   - This kernel performs the E-step of the Expectation-Maximization (EM) algorithm, using the Viterbi algorithm to find the best segmentation of a sentence into subwords based on the maximum log probability.\n",
    "   - The algorithm iterates over each sentence in the corpus and computes the most probable sequence of subwords using character unigram and trigram probabilities.\n",
    "\n",
    "2. **Adjusting for Minimum Subword Length**:\n",
    "   - The `e_step` function now respects both minimum (`min_length`) and maximum (`max_length`) subword lengths.\n",
    "   - For each sentence, we iterate over possible subword segmentations from the previous index (`j`) to the current position (`i`) but limit `j` to ensure subwords are at least `min_length` characters long.\n",
    "\n",
    "3. **Best Path Calculation**:\n",
    "   - For each possible subword in the sentence, we compute its log probability using the function `compute_subword_log_probability`.\n",
    "   - If the log probability of the current segmentation (subword) plus the best log probability of the preceding segment is higher than the current best log probability, we update `best_log_probs[i]` and record the back pointer to the previous index (`back_pointers[i] = j`).\n",
    "   - This ensures that we always keep track of the best possible segmentation path.\n",
    "\n",
    "4. **Log Likelihood Calculation**:\n",
    "   - The total log likelihood for each sentence is computed by summing up the best log probabilities for the entire sentence (`best_log_probs[length]`).\n",
    "   - This log likelihood helps in determining the convergence of the EM algorithm.\n",
    "\n",
    "5. **Backtracking for Tokenization**:\n",
    "   - Once the best log probability path is computed, the function backtracks using the `back_pointers` array to recover the optimal sequence of subwords (tokens).\n",
    "   - This backtracking process produces the list of tokens (`tokens`), which are then appended to the final tokenization output (`tokenizations`).\n",
    "\n",
    "6. **Updating Expected Counts**:\n",
    "   - As we backtrack and recover the subwords, we update the expected counts for character trigrams and unigrams:\n",
    "     - If the subword is at least 3 characters long, its trigram counts are updated in `expected_char_trigram_counts`.\n",
    "     - If the subword is shorter than 3 characters, its unigram counts are updated in `expected_char_unigram_counts`.\n",
    "   - These counts are used in the M-step to update the unigram and trigram probabilities.\n",
    "\n",
    "#### Key Adjustments:\n",
    "- **Minimum Subword Length**:\n",
    "   - We now enforce a minimum subword length (`min_length`) to avoid creating subwords that are too short to be meaningful.\n",
    "   - This ensures the model captures more informative subwords, improving the quality of tokenization.\n",
    "\n",
    "- **Viterbi Path Selection**:\n",
    "   - The use of back pointers ensures that the segmentation process selects the best possible path through the sentence, leading to an optimal tokenization based on the character probabilities.\n",
    "\n",
    "#### Why This Matters:\n",
    "- **Improved Tokenization**:\n",
    "   - By enforcing a minimum subword length, we prevent over-segmentation into overly short tokens, which could lead to a loss of linguistic meaning.\n",
    "   \n",
    "- **Efficient EM Algorithm**:\n",
    "   - The combination of the Viterbi algorithm and the log likelihood computation ensures that the tokenization process is both efficient and probabilistically grounded, making it well-suited for sentence-level segmentation tasks.\n",
    "\n",
    "#### Output:\n",
    "- **Tokenizations**:\n",
    "   - For each sentence, the most probable sequence of subwords is returned.\n",
    "   \n",
    "- **Total Log Likelihood**:\n",
    "   - The overall log likelihood is used to evaluate the current state of the model and track convergence.\n",
    "\n",
    "- **Expected Counts**:\n",
    "   - The expected unigram and trigram counts are updated for the subsequent M-step, where the probabilities are re-estimated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M-step function to update character unigram and trigram probabilities\n",
    "def m_step(expected_char_unigram_counts, expected_char_trigram_counts):\n",
    "    total_unigram_count = sum(expected_char_unigram_counts.values())\n",
    "    total_trigram_count = sum(expected_char_trigram_counts.values())\n",
    "\n",
    "    # Update unigram probabilities\n",
    "    new_char_unigram_probs = {}\n",
    "    for char, count in expected_char_unigram_counts.items():\n",
    "        new_char_unigram_probs[char] = count / total_unigram_count\n",
    "\n",
    "    # Update trigram probabilities\n",
    "    new_char_trigram_probs = {}\n",
    "    for trigram, count in expected_char_trigram_counts.items():\n",
    "        new_char_trigram_probs[trigram] = count / total_trigram_count\n",
    "\n",
    "    return new_char_unigram_probs, new_char_trigram_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-step Function to Update Unigram and Trigram Probabilities\n",
    "\n",
    "#### Step-by-Step Process:\n",
    "\n",
    "1. **Purpose of the M-step**:\n",
    "   - The M-step is responsible for updating the model's parameters — in this case, the unigram and trigram probabilities.\n",
    "   - Based on the expected counts computed during the E-step, the M-step recalculates the probabilities of each character unigram and trigram.\n",
    "\n",
    "2. **Total Counts**:\n",
    "   - First, we calculate the total count of unigrams and trigrams by summing all the values in `expected_char_unigram_counts` and `expected_char_trigram_counts`, respectively.\n",
    "   - These totals are essential to normalize the counts into probabilities.\n",
    "\n",
    "3. **Unigram Probability Update**:\n",
    "   - The function loops through the `expected_char_unigram_counts` dictionary.\n",
    "   - For each character (unigram), we compute its new probability by dividing its count by the total count of unigrams (`total_unigram_count`).\n",
    "   - This ensures that all unigram probabilities sum up to 1, making it a valid probability distribution.\n",
    "\n",
    "4. **Trigram Probability Update**:\n",
    "   - Similarly, the function loops through the `expected_char_trigram_counts` dictionary.\n",
    "   - For each trigram, its probability is updated by dividing its count by the total count of trigrams (`total_trigram_count`).\n",
    "   - Like unigrams, this ensures that the trigram probabilities also form a valid probability distribution.\n",
    "\n",
    "5. **Returning Updated Probabilities**:\n",
    "   - After updating both unigram and trigram probabilities, the function returns the new probability distributions for unigrams and trigrams.\n",
    "\n",
    "#### Why This Matters:\n",
    "- **Probabilistic Re-estimation**:\n",
    "   - This step is critical for re-estimating the probabilities based on the \"soft counts\" generated in the E-step.\n",
    "   - By updating these probabilities, the model iteratively improves its understanding of which unigrams and trigrams are most likely to form meaningful subwords.\n",
    "\n",
    "- **Normalization**:\n",
    "   - By dividing each count by the total count of unigrams or trigrams, the model ensures that the probabilities are properly normalized and represent a valid probability distribution.\n",
    "\n",
    "- **Foundation for the Next E-step**:\n",
    "   - These updated probabilities will be used in the next E-step to compute new segmentations of the sentences. As the probabilities become more accurate, the model's segmentations should improve, leading to better subword tokenization.\n",
    "\n",
    "#### Output:\n",
    "- **Updated Probabilities**:\n",
    "   - The function outputs two dictionaries: `new_char_unigram_probs` and `new_char_trigram_probs`.\n",
    "   - These dictionaries store the updated probabilities for unigrams and trigrams, which will be used in the next iteration of the EM algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Log Likelihood: -14457804.812558465\n",
      "Iteration 2, Log Likelihood: -14249677.480445482\n",
      "Iteration 3, Log Likelihood: -14238696.733951317\n",
      "Iteration 4, Log Likelihood: -14236531.654207421\n",
      "Iteration 5, Log Likelihood: -14235800.421548845\n",
      "Iteration 6, Log Likelihood: -14235565.871720847\n",
      "Iteration 7, Log Likelihood: -14235524.169723144\n",
      "Iteration 8, Log Likelihood: -14235475.954409713\n",
      "Iteration 9, Log Likelihood: -14235375.56963584\n",
      "Iteration 10, Log Likelihood: -14235345.055238042\n",
      "Average tokens per sentence: 21.91\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>SegmentationReason</th>\n",
       "      <th>BPE_Tokens</th>\n",
       "      <th>Merges</th>\n",
       "      <th>Decoded_Sentence</th>\n",
       "      <th>SentencePiece_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>گزشتہ کئی سالوں سے مختلف بحران آتے جاتے رہے</td>\n",
       "      <td>Conjunction</td>\n",
       "      <td>[گ, ز, ش, ت, ہ,  , ک, ئ, ی, 257, ا, ل, و, ں, 2...</td>\n",
       "      <td>{('ے', ' '): 256, (' ', 'س'): 257, ('ت', 256):...</td>\n",
       "      <td>گزشتہ کئی سالوں سے مختلف بحران آتے جاتے رہے</td>\n",
       "      <td>[گزش, تہک, ئیسا, لوں, سےم, ختل, فبح, ران, آتے,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>لیکن حالیہ آٹا ، چینی سمیت دیگر بحران اچانک پی...</td>\n",
       "      <td>Boundary Marker</td>\n",
       "      <td>[259, ک, 260, ح, ا, 259, ہ, 261, ٹ, ا,  , ،,  ...</td>\n",
       "      <td>{(' ', 'ا'): 256, ('ی', ' '): 257, (' ', 'د'):...</td>\n",
       "      <td>لیکن حالیہ آٹا ، چینی سمیت دیگر بحران اچانک پی...</td>\n",
       "      <td>[لیکن, حال, یہآ, ٹا،, چین, یسم, یتد, یگر, بحر,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تاکہ عوام کو ریلیف دیا</td>\n",
       "      <td>Boundary Marker</td>\n",
       "      <td>[ت, ا, ک, ہ,  , ع, و, ا, م,  , ک, و,  , ر, ی, ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>تاکہ عوام کو ریلیف دیا</td>\n",
       "      <td>[تاک, ہعو, امک, وری, لیف, دیا]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>دوسری جانب بجلی ، گیس ، پانی سمیت دیگر بلوں می...</td>\n",
       "      <td>Boundary Marker</td>\n",
       "      <td>[د, و, س, ر, 256, ج, 258, ب, 259, ج, ل, 256, ،...</td>\n",
       "      <td>{('ی', ' '): 256, (' ', 'گ'): 257, ('ا', 'ن'):...</td>\n",
       "      <td>دوسری جانب بجلی ، گیس ، پانی سمیت دیگر بلوں می...</td>\n",
       "      <td>[دوس, ریج, انب, بجل, ی،گ, یس،, پانی, سمی, تدی,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>،صوبائی و وفاقی وزراء نے اپنے اپنے ایوانوں بحر...</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>[،, ص, و, ب, 260, 263,  , و, ف, ا, ق, 263, ز, ...</td>\n",
       "      <td>{('ے', ' '): 256, ('ی', ' '): 257, (' ', 'ک'):...</td>\n",
       "      <td>،صوبائی و وفاقی وزراء نے اپنے اپنے ایوانوں بحر...</td>\n",
       "      <td>[،صو, بائ, یوو, فاق, یوز, راء, نےا, پنے, اپن, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75903</th>\n",
       "      <td>کیا22سالہ جدوجہداورقربانیوں کامنطقی مقصدفقت یہ...</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>[ک, ی, ا, 2, 2, س, ا, ل, ہ,  , ج, د, و, ج, ہ, ...</td>\n",
       "      <td>{('ی', ' '): 256, ('ت', ' '): 257}</td>\n",
       "      <td>کیا22سالہ جدوجہداورقربانیوں کامنطقی مقصدفقت یہ...</td>\n",
       "      <td>[کیا, 22س, الہ, جدو, جہد, اور, قرب, انی, وںک, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75904</th>\n",
       "      <td>اب دیکھنایہ ہے</td>\n",
       "      <td>Boundary Marker</td>\n",
       "      <td>[ا, ب,  , د, ی, ک, ھ, ن, ا, ی, ہ,  , ہ, ے]</td>\n",
       "      <td>{}</td>\n",
       "      <td>اب دیکھنایہ ہے</td>\n",
       "      <td>[ابد, یکھ, نای, ہہے]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75905</th>\n",
       "      <td>کہ عمران خان نے الیکشن سے پہلے جوبھڑکیں ماری و...</td>\n",
       "      <td>Boundary Marker</td>\n",
       "      <td>[ک, 258, ع, م, ر, 259, خ, 259, ن, 256, ا, ل, ی...</td>\n",
       "      <td>{('ے', ' '): 256, ('ن', ' '): 257, ('ہ', ' '):...</td>\n",
       "      <td>کہ عمران خان نے الیکشن سے پہلے جوبھڑکیں ماری و...</td>\n",
       "      <td>[کہع, مرا, نخا, ننے, الی, کشن, سےپ, ہلے, جوب, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75906</th>\n",
       "      <td>یاصرف عوام کواذیت دے کے رخصت ہوجاتے ہیں</td>\n",
       "      <td>Boundary Marker</td>\n",
       "      <td>[ی, ا, ص, ر, ف,  , ع, 257, م,  , ک, 257, ذ, ی,...</td>\n",
       "      <td>{('ے', ' '): 256, ('و', 'ا'): 257, ('ت', ' '):...</td>\n",
       "      <td>یاصرف عوام کواذیت دے کے رخصت ہوجاتے ہیں</td>\n",
       "      <td>[یاص, رفع, وام, کوا, ذیت, دےکے, رخص, تہو, جات,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75907</th>\n",
       "      <td>جو تیرے وعدے پہ اعتبار کیا</td>\n",
       "      <td>Boundary Marker</td>\n",
       "      <td>[ج, و,  , ت, ی, ر, 256, و, ع, د, 256, پ, ہ,  ,...</td>\n",
       "      <td>{('ے', ' '): 256}</td>\n",
       "      <td>جو تیرے وعدے پہ اعتبار کیا</td>\n",
       "      <td>[جوت, یرے, وعد, ےپہ, اعت, بار, کیا]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75908 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence SegmentationReason  \\\n",
       "0            گزشتہ کئی سالوں سے مختلف بحران آتے جاتے رہے        Conjunction   \n",
       "1      لیکن حالیہ آٹا ، چینی سمیت دیگر بحران اچانک پی...    Boundary Marker   \n",
       "2                                 تاکہ عوام کو ریلیف دیا    Boundary Marker   \n",
       "3      دوسری جانب بجلی ، گیس ، پانی سمیت دیگر بلوں می...    Boundary Marker   \n",
       "4      ،صوبائی و وفاقی وزراء نے اپنے اپنے ایوانوں بحر...        Punctuation   \n",
       "...                                                  ...                ...   \n",
       "75903  کیا22سالہ جدوجہداورقربانیوں کامنطقی مقصدفقت یہ...        Punctuation   \n",
       "75904                                     اب دیکھنایہ ہے    Boundary Marker   \n",
       "75905  کہ عمران خان نے الیکشن سے پہلے جوبھڑکیں ماری و...    Boundary Marker   \n",
       "75906            یاصرف عوام کواذیت دے کے رخصت ہوجاتے ہیں    Boundary Marker   \n",
       "75907                         جو تیرے وعدے پہ اعتبار کیا    Boundary Marker   \n",
       "\n",
       "                                              BPE_Tokens  \\\n",
       "0      [گ, ز, ش, ت, ہ,  , ک, ئ, ی, 257, ا, ل, و, ں, 2...   \n",
       "1      [259, ک, 260, ح, ا, 259, ہ, 261, ٹ, ا,  , ،,  ...   \n",
       "2      [ت, ا, ک, ہ,  , ع, و, ا, م,  , ک, و,  , ر, ی, ...   \n",
       "3      [د, و, س, ر, 256, ج, 258, ب, 259, ج, ل, 256, ،...   \n",
       "4      [،, ص, و, ب, 260, 263,  , و, ف, ا, ق, 263, ز, ...   \n",
       "...                                                  ...   \n",
       "75903  [ک, ی, ا, 2, 2, س, ا, ل, ہ,  , ج, د, و, ج, ہ, ...   \n",
       "75904         [ا, ب,  , د, ی, ک, ھ, ن, ا, ی, ہ,  , ہ, ے]   \n",
       "75905  [ک, 258, ع, م, ر, 259, خ, 259, ن, 256, ا, ل, ی...   \n",
       "75906  [ی, ا, ص, ر, ف,  , ع, 257, م,  , ک, 257, ذ, ی,...   \n",
       "75907  [ج, و,  , ت, ی, ر, 256, و, ع, د, 256, پ, ہ,  ,...   \n",
       "\n",
       "                                                  Merges  \\\n",
       "0      {('ے', ' '): 256, (' ', 'س'): 257, ('ت', 256):...   \n",
       "1      {(' ', 'ا'): 256, ('ی', ' '): 257, (' ', 'د'):...   \n",
       "2                                                     {}   \n",
       "3      {('ی', ' '): 256, (' ', 'گ'): 257, ('ا', 'ن'):...   \n",
       "4      {('ے', ' '): 256, ('ی', ' '): 257, (' ', 'ک'):...   \n",
       "...                                                  ...   \n",
       "75903                 {('ی', ' '): 256, ('ت', ' '): 257}   \n",
       "75904                                                 {}   \n",
       "75905  {('ے', ' '): 256, ('ن', ' '): 257, ('ہ', ' '):...   \n",
       "75906  {('ے', ' '): 256, ('و', 'ا'): 257, ('ت', ' '):...   \n",
       "75907                                  {('ے', ' '): 256}   \n",
       "\n",
       "                                        Decoded_Sentence  \\\n",
       "0            گزشتہ کئی سالوں سے مختلف بحران آتے جاتے رہے   \n",
       "1      لیکن حالیہ آٹا ، چینی سمیت دیگر بحران اچانک پی...   \n",
       "2                                 تاکہ عوام کو ریلیف دیا   \n",
       "3      دوسری جانب بجلی ، گیس ، پانی سمیت دیگر بلوں می...   \n",
       "4      ،صوبائی و وفاقی وزراء نے اپنے اپنے ایوانوں بحر...   \n",
       "...                                                  ...   \n",
       "75903  کیا22سالہ جدوجہداورقربانیوں کامنطقی مقصدفقت یہ...   \n",
       "75904                                     اب دیکھنایہ ہے   \n",
       "75905  کہ عمران خان نے الیکشن سے پہلے جوبھڑکیں ماری و...   \n",
       "75906            یاصرف عوام کواذیت دے کے رخصت ہوجاتے ہیں   \n",
       "75907                         جو تیرے وعدے پہ اعتبار کیا   \n",
       "\n",
       "                                    SentencePiece_Tokens  \n",
       "0      [گزش, تہک, ئیسا, لوں, سےم, ختل, فبح, ران, آتے,...  \n",
       "1      [لیکن, حال, یہآ, ٹا،, چین, یسم, یتد, یگر, بحر,...  \n",
       "2                         [تاک, ہعو, امک, وری, لیف, دیا]  \n",
       "3      [دوس, ریج, انب, بجل, ی،گ, یس،, پانی, سمی, تدی,...  \n",
       "4      [،صو, بائ, یوو, فاق, یوز, راء, نےا, پنے, اپن, ...  \n",
       "...                                                  ...  \n",
       "75903  [کیا, 22س, الہ, جدو, جہد, اور, قرب, انی, وںک, ...  \n",
       "75904                               [ابد, یکھ, نای, ہہے]  \n",
       "75905  [کہع, مرا, نخا, ننے, الی, کشن, سےپ, ہلے, جوب, ...  \n",
       "75906  [یاص, رفع, وام, کوا, ذیت, دےکے, رخص, تہو, جات,...  \n",
       "75907                [جوت, یرے, وعد, ےپہ, اعت, بار, کیا]  \n",
       "\n",
       "[75908 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EM algorithm iterations\n",
    "max_iterations = 10  \n",
    "likelihood_threshold = 1e-4\n",
    "previous_likelihood = None\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    tokenizations, total_log_likelihood, expected_char_unigram_counts, expected_char_trigram_counts = e_step(\n",
    "        sentences, char_unigram_probs, char_trigram_probs, min_subword_length, max_subword_length)\n",
    "    char_unigram_probs, char_trigram_probs = m_step(expected_char_unigram_counts, expected_char_trigram_counts)\n",
    "    \n",
    "    print(f\"Iteration {iteration + 1}, Log Likelihood: {total_log_likelihood}\")\n",
    "\n",
    "    # Check for convergence\n",
    "    if previous_likelihood is not None and abs(total_log_likelihood - previous_likelihood) < likelihood_threshold:\n",
    "        print(\"Converged.\")\n",
    "        break\n",
    "    previous_likelihood = total_log_likelihood\n",
    "\n",
    "# Assign the final tokenizations to the DataFrame\n",
    "df['SentencePiece_Tokens'] = tokenizations\n",
    "\n",
    "# Evaluate the tokenization\n",
    "def evaluate_tokenization(df):\n",
    "    total_tokens = sum(df['SentencePiece_Tokens'].apply(len))\n",
    "    total_sentences = len(df)\n",
    "    avg_tokens_per_sentence = total_tokens / total_sentences\n",
    "    print(f\"Average tokens per sentence: {avg_tokens_per_sentence:.2f}\")\n",
    "\n",
    "evaluate_tokenization(df)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Explanation: EM Algorithm Iterations and Tokenization Assignment\n",
    "\n",
    "#### Overview:\n",
    "This kernel implements the main loop for the Expectation-Maximization (EM) algorithm, which iteratively refines subword tokenizations using unigram and trigram probabilities.\n",
    "\n",
    "#### Step-by-Step Process:\n",
    "\n",
    "1. **EM Algorithm Parameters**:\n",
    "   - `max_iterations`: The maximum number of iterations the EM algorithm will run. Here, it's set to 10.\n",
    "   - `likelihood_threshold`: The threshold for checking the convergence of the algorithm. If the difference in log-likelihood between iterations is below this threshold, the algorithm will stop early, as it is considered to have converged.\n",
    "   - `previous_likelihood`: Used to track the log-likelihood from the previous iteration for convergence checking.\n",
    "\n",
    "2. **EM Loop**:\n",
    "   - For each iteration (up to `max_iterations`), the following steps are performed:\n",
    "     - **E-step**: The `e_step` function computes the tokenizations and expected counts for unigrams and trigrams based on current probabilities. It also calculates the total log-likelihood.\n",
    "     - **M-step**: The `m_step` function updates the unigram and trigram probabilities based on the expected counts computed in the E-step.\n",
    "     - **Log Likelihood Check**: After each iteration, the log-likelihood is printed. If the change in log-likelihood is smaller than `likelihood_threshold`, the algorithm is considered to have converged, and the loop breaks early.\n",
    "   \n",
    "3. **Convergence**:\n",
    "   - The convergence condition checks if the absolute difference between the current log-likelihood (`total_log_likelihood`) and the previous log-likelihood (`previous_likelihood`) is smaller than the `likelihood_threshold`. If true, the algorithm converges and stops early.\n",
    "   \n",
    "4. **Assigning Tokenizations to DataFrame**:\n",
    "   - After completing the EM algorithm, the resulting tokenizations are assigned to the DataFrame (`df['SentencePiece_Tokens']`).\n",
    "   - The `tokenizations` list contains the final tokenized output for each sentence.\n",
    "\n",
    "5. **Evaluate Tokenization**:\n",
    "   - The `evaluate_tokenization` function calculates the average number of tokens per sentence:\n",
    "     - `total_tokens`: Sum of the length (number of tokens) for all tokenized sentences.\n",
    "     - `total_sentences`: Number of sentences in the dataset.\n",
    "     - **Average Tokens per Sentence**: Calculated by dividing `total_tokens` by `total_sentences`. The result is printed to help evaluate the efficiency of the tokenization process.\n",
    "\n",
    "6. **Final DataFrame**:\n",
    "   - The DataFrame `df` now contains a new column (`SentencePiece_Tokens`) with the SentencePiece-style tokenizations for each sentence.\n",
    "   - This updated DataFrame can be further used for analysis, model training, or other NLP tasks.\n",
    "\n",
    "#### Why This Matters:\n",
    "- **Iterative Improvement**:\n",
    "   - The EM algorithm ensures that the subword tokenization improves iteratively as unigram and trigram probabilities are refined, leading to better segmentation results.\n",
    "   \n",
    "- **Convergence Check**:\n",
    "   - By checking for convergence, the algorithm can stop early if the log-likelihood stabilizes, saving unnecessary computations.\n",
    "\n",
    "- **Final Tokenization**:\n",
    "   - The resulting tokenizations provide a meaningful subword segmentation of Urdu text, which is crucial for further language processing tasks like text generation, language modeling, or classification.\n",
    "\n",
    "#### Output:\n",
    "- **Log Likelihood**: Log-likelihood values are printed for each iteration, showing how well the model fits the data.\n",
    "- **Average Tokens per Sentence**: The function prints the average number of tokens per sentence, giving insight into the segmentation efficiency.\n",
    "- **Updated DataFrame**: The final DataFrame `df` now includes the `SentencePiece_Tokens` column, which contains the tokenized version of each sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Segmented_Sentences'] = df['Sentence'].apply(urdu_sentence_segmentation)\n",
    "all_segmented_sentences = [sentence[0] for segments in df['Segmented_Sentences'] for sentence in segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words per segmented sentence: 17.56\n",
      "Average number of words per original sentence: 19.91\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the number of words in each segmented sentence\n",
    "segmented_sentence_lengths = [len(sentence.split()) for sentence in all_segmented_sentences]\n",
    "\n",
    "# Compute the average sentence length\n",
    "average_segmented_sentence_length = sum(segmented_sentence_lengths) / len(segmented_sentence_lengths)\n",
    "\n",
    "print(f\"Average number of words per segmented sentence: {average_segmented_sentence_length:.2f}\")\n",
    "\n",
    "# For comparison, compute the average sentence length in the original text (if applicable)\n",
    "original_sentences = df['Sentence'].tolist()\n",
    "original_sentence_lengths = [len(sentence.split()) for sentence in original_sentences]\n",
    "average_original_sentence_length = sum(original_sentence_lengths) / len(original_sentence_lengths)\n",
    "\n",
    "print(f\"Average number of words per original sentence: {average_original_sentence_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram of segmented sentence lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(segmented_sentence_lengths, bins=range(0, max(segmented_sentence_lengths) + 1, 1), edgecolor='black')\n",
    "plt.title('Distribution of Segmented Sentence Lengths')\n",
    "plt.xlabel('Number of Words per Sentence')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences starting with a conjunction: 19174 (25.26%)\n",
      "Sentences ending with a boundary marker: 56384 (74.28%)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of segmented sentences\n",
    "segmented_sentences = all_segmented_sentences\n",
    "\n",
    "# Count sentences starting with a conjunction\n",
    "conjunction_starts = sum(1 for sentence in segmented_sentences if sentence.split()[0] in conjunctions)\n",
    "\n",
    "# Count sentences ending with a boundary marker\n",
    "boundary_marker_ends = sum(1 for sentence in segmented_sentences if sentence.split()[-1] in boundary_markers)\n",
    "\n",
    "total_sentences = len(segmented_sentences)\n",
    "\n",
    "print(f\"Sentences starting with a conjunction: {conjunction_starts} ({(conjunction_starts/total_sentences)*100:.2f}%)\")\n",
    "print(f\"Sentences ending with a boundary marker: {boundary_marker_ends} ({(boundary_marker_ends/total_sentences)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interpretation: A high percentage of sentences starting with conjunctions might indicate over-segmentation.\n",
    "- Considerations: In natural Urdu text, sentences typically do not start with conjunctions unless stylistically intended."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentence_segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
